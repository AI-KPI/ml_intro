{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "decision_tree_classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "toc": {
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fkmgUpWPu4r9"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/YKochura/ml_basics/blob/main/decision_tree_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqWoV8DBu0nN"
      },
      "source": [
        "## Дерево рішень для класифікації у Python\n",
        "\n",
        "\n",
        "Дерево рішень - це **контрольована (supervised)** модель машинного навчання, яка може використовуватися як для **класифікації**, так і для **регресії**. По суті, дерево рішень використовує деревоподібну структуру для прогнозування вихідного значення для даного вхідного прикладу. У дереві кожен шлях від кореневого вузла до листового вузла представляє *шлях прийняття рішення (decision path)*, який закінчується передбаченим значенням.\n",
        "\n",
        "Простий приклад виглядає наступним чином:\n",
        "![caption](https://github.com/YKochura/ml_basics/blob/main/figures/decision_tree.png?raw=1)\n",
        "\n",
        "Дерева рішень мають багато переваг. Наприклад, їх легко зрозуміти, а їх рішення легко інтерпретувати. Крім того, вони не вимагають складної підготовки даних. Можна знайти більш широкий перелік їх переваг та недоліків [тут](http://scikit-learn.org/stable/modules/tree.html).\n",
        "\n",
        "### Алгоритм навчання CART \n",
        "Для підготовки дерева рішень можна використовувати різні алгоритми. У цьому нотбуці ми зосередимось на алгоритмі *CART* (Classification and Regression Trees) для *класифікації*. Алгоритм CART створює *бінарне дерево*, в якому кожен нелистовий вузол має рівно двох дітей (що відповідає відповіді так/ні).\n",
        "\n",
        "Враховуючи набір навчальних прикладів та їх мітки, алгоритм неодноразово розбиває навчальні приклади $D$ на дві підмножини $D_{left}, D_{right}$ використовуючи деяку ознаку $f$ та поріг ознаки $t_f$ такі, що приклади з однаковими мітками групуються разом. На кожному вузлі алгоритм вибирає розділ $\\theta = (f, t_f)$, який видає найчистіші підмножини, зважені за їх розміром. Чистота вимірюється за допомогою *коефіцієнту Джині (Gini impurity, не плутати з Gini coefficient)*.\n",
        "\n",
        "Отже, на кожному кроці алгоритм вибирає параметри $\\theta$, які мінімізують наступну функцію витрат:\n",
        "\n",
        "\\begin{equation}\n",
        "J(D, \\theta) = \\frac{n_{left}}{n_{total}} G_{left} + \\frac{n_{right}}{n_{total}} G_{right}\n",
        "\\end{equation}\n",
        "\n",
        "- $D$: навчальні приклади, що залишились\n",
        "- $n_{total}$ : кількість навчальних прикладів, що залишились\n",
        "- $\\theta = (f, t_f)$: ознака та поріг ознаки\n",
        "- $n_{left}/n_{right}$: кількість прикладів у лівій / правій підмножині\n",
        "- $G_{left}/G_{right}$: Gini impurity лівої / правої підмножини\n",
        "\n",
        "Цей крок повторюється рекурсивно, доки не буде досягнута *максимально допустима глибина* або поточна кількість зразків $n_{total}$ не опуститься нижче деякої мінімальної кількості. Оригінальні рівняння можна знайти [тут](http://scikit-learn.org/stable/modules/tree.html).\n",
        "\n",
        "Після побудови дерева нові приклади можна класифікувати шляхом навігації по дереву, перевіряючи на кожному вузлі відповідну ознаку, поки не буде досягнутий листовий вузол/прогноз.\n",
        "\n",
        "\n",
        "### Коефіцієнт Джині (Gini Impurity)\n",
        "\n",
        "Враховуючи $K$ різних класифікаційних значень $k \\in \\{1, ..., K\\}$ Gini impurity вузла $m$ бчислюється наступним чином:\n",
        "\n",
        "\\begin{equation}\n",
        "G_m = 1 - \\sum_{k=1}^{K} (p_{m,k})^2\n",
        "\\end{equation}\n",
        "\n",
        "де $p_{m, k}$ - це частка прикладів навчання з класом $k$ серед усіх прикладів навчання у вузлі $m$.\n",
        "\n",
        "Gini impurity можна використовувати для оцінки того, на скільки добре виконано розподіл. Розподіл ділить заданий набір навчальних прикладів на дві групи. Gini вимірює, наскільки \"змішані\" отримані групи. Ідеальне розділення (тобто кожна група містить лише зразки одного класу) відповідає Gini impurity = 0. Якщо отримані групи містять однаково багато зразків кожного класу, Gini impurity досягне найвищого значення 0,5.\n",
        "\n",
        "### Застереження\n",
        "\n",
        "Без регуляризації дерева прийняття рішень, ймовірно, виникне перенавчання. Цього можна запобігти, використовуючи такі методи, як *обрізка*, або надаючи максимально дозволену глибину дерева та/або мінімальну кількість зразків, необхідних для подальшого розділення вузла."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:18.442511Z",
          "start_time": "2018-04-09T12:50:18.426963Z"
        },
        "id": "RViWXBX-u0nY"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "np.random.seed(123)\n",
        "\n",
        "% matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSAoLcqDu0na"
      },
      "source": [
        "## Набір даних\n",
        "\n",
        "Набір даних iris поєднує в собі 150 прикладів 3 різних видів квітів ірису (Setosa, Versicolour та Virginica). Кожен приклад описується чотирма атрибутами: довжина чашолистка (см), ширина чашолистка (см), довжина пелюстки (см), ширина пелюстки (см)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:18.959842Z",
          "start_time": "2018-04-09T12:50:18.951915Z"
        },
        "id": "sHDU4zADu0nb"
      },
      "source": [
        "iris = load_iris()\n",
        "\n",
        "X, y = iris.data, iris.target"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:19.202874Z",
          "start_time": "2018-04-09T12:50:19.188064Z"
        },
        "id": "YOhW9Xs1u0nc",
        "outputId": "36e0847a-a1b1-4aeb-c3fc-57253141a30c"
      },
      "source": [
        "# Split the data into a training and test set\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
        "\n",
        "print(f'Shape X_train: {X_train.shape}')\n",
        "print(f'Shape y_train: {y_train.shape}')\n",
        "print(f'Shape X_test: {X_test.shape}')\n",
        "print(f'Shape y_test: {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape X_train: (112, 4)\n",
            "Shape y_train: (112,)\n",
            "Shape X_test: (38, 4)\n",
            "Shape y_test: (38,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REtfFpY_u0nf"
      },
      "source": [
        "## Клас дерева рішень\n",
        "\n",
        "Деякі частини коду були взяті з [цього туторіалу](https://machinelearningmastery.com/implement-decision-tree-algorithm-scratch-python/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:20.706089Z",
          "start_time": "2018-04-09T12:50:20.093228Z"
        },
        "id": "hWKd7qbfu0nh"
      },
      "source": [
        "class DecisionTree:\n",
        "    \"\"\"\n",
        "    Decision tree for classification\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.root_dict = None\n",
        "        self.tree_dict = None\n",
        "\n",
        "    def split_dataset(self, X, y, feature_idx, threshold):\n",
        "        \"\"\"\n",
        "        Splits dataset X into two subsets, according to a given feature\n",
        "        and feature threshold.\n",
        "\n",
        "        Args:\n",
        "            X: 2D numpy array with data samples\n",
        "            y: 1D numpy array with labels\n",
        "            feature_idx: int, index of feature used for splitting the data\n",
        "            threshold: float, threshold used for splitting the data\n",
        "\n",
        "        Returns:\n",
        "            splits: dict containing the left and right subsets\n",
        "            and their labels\n",
        "        \"\"\"\n",
        "\n",
        "        left_idx = np.where(X[:, feature_idx] < threshold)\n",
        "        right_idx = np.where(X[:, feature_idx] >= threshold)\n",
        "\n",
        "        left_subset = X[left_idx]\n",
        "        y_left = y[left_idx]\n",
        "\n",
        "        right_subset = X[right_idx]\n",
        "        y_right = y[right_idx]\n",
        "\n",
        "        splits = {\n",
        "        'left': left_subset,\n",
        "        'y_left': y_left,\n",
        "        'right': right_subset,\n",
        "        'y_right': y_right,\n",
        "        }\n",
        "\n",
        "        return splits\n",
        "\n",
        "    def gini_impurity(self, y_left, y_right, n_left, n_right):\n",
        "        \"\"\"\n",
        "        Computes Gini impurity of a split.\n",
        "\n",
        "        Args:\n",
        "            y_left, y_right: target values of samples in left/right subset\n",
        "            n_left, n_right: number of samples in left/right subset\n",
        "\n",
        "        Returns:\n",
        "            gini_left: float, Gini impurity of left subset\n",
        "            gini_right: gloat, Gini impurity of right subset\n",
        "        \"\"\"\n",
        "\n",
        "        n_total = n_left + n_left\n",
        "\n",
        "        score_left, score_right = 0, 0\n",
        "        gini_left, gini_right = 0, 0\n",
        "\n",
        "        if n_left != 0:\n",
        "            for c in range(self.n_classes):\n",
        "                # For each class c, compute fraction of samples with class c\n",
        "                p_left = len(np.where(y_left == c)[0]) / n_left\n",
        "                score_left += p_left * p_left\n",
        "            gini_left = 1 - score_left\n",
        "\n",
        "        if n_right != 0:\n",
        "            for c in range(self.n_classes):\n",
        "                p_right = len(np.where(y_right == c)[0]) / n_right\n",
        "                score_right += p_right * p_right\n",
        "            gini_right = 1 - score_right\n",
        "\n",
        "        return gini_left, gini_right\n",
        "\n",
        "    def get_cost(self, splits):\n",
        "        \"\"\"\n",
        "        Computes cost of a split given the Gini impurity of\n",
        "        the left and right subset and the sizes of the subsets\n",
        "        \n",
        "        Args:\n",
        "            splits: dict, containing params of current split\n",
        "        \"\"\"\n",
        "        y_left = splits['y_left']\n",
        "        y_right = splits['y_right']\n",
        "\n",
        "        n_left = len(y_left)\n",
        "        n_right = len(y_right)\n",
        "        n_total = n_left + n_right\n",
        "\n",
        "        gini_left, gini_right = self.gini_impurity(y_left, y_right, n_left, n_right)\n",
        "        cost = (n_left / n_total) * gini_left + (n_right / n_total) * gini_right\n",
        "\n",
        "        return cost\n",
        "\n",
        "    def find_best_split(self, X, y):\n",
        "        \"\"\"\n",
        "        Finds the best feature and feature index to split dataset X into\n",
        "        two groups. Checks every value of every attribute as a candidate\n",
        "        split.\n",
        "\n",
        "        Args:\n",
        "            X: 2D numpy array with data samples\n",
        "            y: 1D numpy array with labels\n",
        "\n",
        "        Returns:\n",
        "            best_split_params: dict, containing parameters of the best split\n",
        "        \"\"\"\n",
        "\n",
        "        n_samples, n_features = X.shape\n",
        "\n",
        "        best_feature_idx, best_threshold, best_cost, best_splits = np.inf, np.inf, np.inf, None\n",
        "\n",
        "        for feature_idx in range(n_features):\n",
        "            for i in range(n_samples):\n",
        "                current_sample = X[i]\n",
        "                threshold = current_sample[feature_idx]\n",
        "                splits = self.split_dataset(X, y, feature_idx, threshold)\n",
        "                cost = self.get_cost(splits)\n",
        "\n",
        "                if cost < best_cost:\n",
        "                    best_feature_idx = feature_idx\n",
        "                    best_threshold = threshold\n",
        "                    best_cost = cost\n",
        "                    best_splits = splits\n",
        "\n",
        "        best_split_params = {\n",
        "            'feature_idx': best_feature_idx,\n",
        "            'threshold': best_threshold,\n",
        "            'cost': best_cost,\n",
        "            'left': best_splits['left'],\n",
        "            'y_left': best_splits['y_left'],\n",
        "            'right': best_splits['right'],\n",
        "            'y_right': best_splits['y_right'],\n",
        "        }\n",
        "\n",
        "        return best_split_params\n",
        "\n",
        "\n",
        "    def build_tree(self, node_dict, depth, max_depth, min_samples):\n",
        "        \"\"\"\n",
        "        Builds the decision tree in a recursive fashion.\n",
        "\n",
        "        Args:\n",
        "            node_dict: dict, representing the current node\n",
        "            depth: int, depth of current node in the tree\n",
        "            max_depth: int, maximum allowed tree depth\n",
        "            min_samples: int, minimum number of samples needed to split a node further\n",
        "\n",
        "        Returns:\n",
        "            node_dict: dict, representing the full subtree originating from current node\n",
        "        \"\"\"\n",
        "        left_samples = node_dict['left']\n",
        "        right_samples = node_dict['right']\n",
        "        y_left_samples = node_dict['y_left']\n",
        "        y_right_samples = node_dict['y_right']\n",
        "\n",
        "        if len(y_left_samples) == 0 or len(y_right_samples) == 0:\n",
        "            node_dict[\"left_child\"] = node_dict[\"right_child\"] = self.create_terminal_node(np.append(y_left_samples, y_right_samples))\n",
        "            return None\n",
        "\n",
        "        if depth >= max_depth:\n",
        "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
        "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
        "            return None\n",
        "\n",
        "        if len(right_samples) < min_samples:\n",
        "            node_dict[\"right_child\"] = self.create_terminal_node(y_right_samples)\n",
        "        else:\n",
        "            node_dict[\"right_child\"] = self.find_best_split(right_samples, y_right_samples)\n",
        "            self.build_tree(node_dict[\"right_child\"], depth+1, max_depth, min_samples)\n",
        "\n",
        "        if len(left_samples) < min_samples:\n",
        "            node_dict[\"left_child\"] = self.create_terminal_node(y_left_samples)\n",
        "        else:\n",
        "            node_dict[\"left_child\"] = self.find_best_split(left_samples, y_left_samples)\n",
        "            self.build_tree(node_dict[\"left_child\"], depth+1, max_depth, min_samples)\n",
        "\n",
        "        return node_dict\n",
        "\n",
        "    def create_terminal_node(self, y):\n",
        "        \"\"\"\n",
        "        Creates a terminal node.\n",
        "        Given a set of labels the most common label is computed and\n",
        "        set as the classification value of the node.\n",
        "\n",
        "        Args:\n",
        "            y: 1D numpy array with labels\n",
        "        Returns:\n",
        "            classification: int, predicted class\n",
        "        \"\"\"\n",
        "        classification = max(set(y), key=list(y).count)\n",
        "        return classification\n",
        "\n",
        "    def train(self, X, y, max_depth, min_samples):\n",
        "        \"\"\"\n",
        "        Fits decision tree on a given dataset.\n",
        "\n",
        "        Args:\n",
        "            X: 2D numpy array with data samples\n",
        "            y: 1D numpy array with labels\n",
        "            max_depth: int, maximum allowed tree depth\n",
        "            min_samples: int, minimum number of samples needed to split a node further\n",
        "        \"\"\"\n",
        "        self.n_classes = len(set(y))\n",
        "        self.root_dict = self.find_best_split(X, y)\n",
        "        self.tree_dict = self.build_tree(self.root_dict, 1, max_depth, min_samples)\n",
        "\n",
        "    def predict(self, X, node):\n",
        "        \"\"\"\n",
        "        Predicts the class for a given input example X.\n",
        "\n",
        "        Args:\n",
        "            X: 1D numpy array, input example\n",
        "            node: dict, representing trained decision tree\n",
        "\n",
        "        Returns:\n",
        "            prediction: int, predicted class\n",
        "        \"\"\"\n",
        "        feature_idx = node['feature_idx']\n",
        "        threshold = node['threshold']\n",
        "\n",
        "        if X[feature_idx] < threshold:\n",
        "            if isinstance(node['left_child'], (int, np.integer)):\n",
        "                return node['left_child']\n",
        "            else:\n",
        "                prediction = self.predict(X, node['left_child'])\n",
        "        elif X[feature_idx] >= threshold:\n",
        "            if isinstance(node['right_child'], (int, np.integer)):\n",
        "                return node['right_child']\n",
        "            else:\n",
        "                prediction = self.predict(X, node['right_child'])\n",
        "\n",
        "        return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:19:30.968801Z",
          "start_time": "2018-04-09T12:19:30.963691Z"
        },
        "id": "3KmKTjFOu0nj"
      },
      "source": [
        "## Ініціалізація та навчання дерева рішень"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:21.291025Z",
          "start_time": "2018-04-09T12:50:21.235025Z"
        },
        "id": "MyjF-5Pwu0nj"
      },
      "source": [
        "tree = DecisionTree()\n",
        "tree.train(X_train, y_train, max_depth=2, min_samples=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSw4fd4-u0nk"
      },
      "source": [
        "## Вивід структури дерева рішень"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:24.127183Z",
          "start_time": "2018-04-09T12:50:24.115505Z"
        },
        "id": "lVZz9E5Iu0nk"
      },
      "source": [
        "def print_tree(node, depth=0):\n",
        "    if isinstance(node, (int, np.integer)):\n",
        "        print(f\"{depth * '  '}predicted class: {iris.target_names[node]}\")\n",
        "    else:\n",
        "        print(f\"{depth * '  '}{iris.feature_names[node['feature_idx']]} < {node['threshold']}, \"\n",
        "             f\"cost of split: {round(node['cost'], 3)}\")\n",
        "        print_tree(node[\"left_child\"], depth+1)\n",
        "        print_tree(node[\"right_child\"], depth+1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:24.818384Z",
          "start_time": "2018-04-09T12:50:24.809082Z"
        },
        "id": "3sPPSbUpu0nn",
        "outputId": "6265ec74-4a54-439a-ee8e-f3aabe63db29"
      },
      "source": [
        "print_tree(tree.tree_dict)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "petal length (cm) < 3.0, cost of split: 0.346\n",
            "  sepal length (cm) < 5.4, cost of split: 0.0\n",
            "    predicted class: setosa\n",
            "    predicted class: setosa\n",
            "  petal width (cm) < 1.8, cost of split: 0.097\n",
            "    predicted class: versicolor\n",
            "    predicted class: virginica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux-3arhhu0np"
      },
      "source": [
        "## Тестування дерева рішень"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2018-04-09T12:50:28.307583Z",
          "start_time": "2018-04-09T12:50:28.284228Z"
        },
        "id": "aQJu6ZlAu0np",
        "outputId": "ff37f783-8429-4431-80f7-4cc132fc7430"
      },
      "source": [
        "all_predictions = []\n",
        "for i in range(X_test.shape[0]):\n",
        "    result = tree.predict(X_test[i], tree.tree_dict)\n",
        "    all_predictions.append(y_test[i] == result)\n",
        "\n",
        "print(f\"Accuracy on test set: {sum(all_predictions) / len(all_predictions)}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy on test set: 0.9473684210526315\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5NekKgAu0nq"
      },
      "source": [
        "## Переваги та недоліки\n",
        "\n",
        "### Переваги:\n",
        " - Простота, зручність візуалізації.\n",
        " - Нескладна підготовка даних, не виманає нормалізації, однак модель не підтримує відсутні значення.\n",
        " - Вартість прогнозування є логарифмічною функцією від кількості точок даних;\n",
        " - Здатність одробляти як числові так і категоріальні типи даних. Інші методи, як правило, спеціалізуються на аналізі наборів даних, які мають лише один тип змінних.\n",
        " - Здатність вирішувати проблеми з кількома виходами.\n",
        " - Використовується модель білої коробки. (Результат моделі є відносно простим для інтерпретації.\n",
        " - Можна перевірити модель за допомогою статистичних тестів. Це дає змогу враховувати надійність моделі.\n",
        " \n",
        "### Недоліки\n",
        " - Перенавчання.\n",
        " - Дерева рішень можуть бути нестабільними, оскільки невеликі зміни даних можуть призвести до генерування зовсім іншого дерева. Цю проблему пом’якшує використання дерев рішень у межах ансамблю.\n",
        " - Відомо, що проблема вивчення дерева оптимальних рішень є NP-повною за кількох аспектів оптимальності і навіть для простих концепцій. Отже, практичні алгоритми навчання на дереві рішень засновані на евристичних алгоритмах, таких як жадібний алгоритм, де локально оптимальні рішення приймаються на кожному вузлі. Такі алгоритми не можуть гарантувати повернення глобально оптимального дерева рішень. Це можна пом'якшити шляхом навчання кількох дерев в ансамблі, який навчається, де функції та вибірки вибірково вибираються із заміною.\n",
        " - Є поняття, які важко вивчити, оскільки дерева рішень не виражають їх легко, наприклад, XOR, проблеми паритету чи мультиплексора."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wEplzClWVTUW"
      },
      "source": [
        "# Джерело\n",
        "[Decision tree for classification](https://github.com/zotroneneis/machine_learning_basics/blob/master/decision_tree_classification.ipynb)\n",
        "\n",
        "Переклад: Осіпов Іван, Бровченко Анастасія студенти ІО-01мп, 2020 рік"
      ]
    }
  ]
}